{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRR 指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRR (Mean Reciprocal Rank) 是评估信息检索系统的重要指标之一。让我解释下：\n",
    "\n",
    "1. **定义**\n",
    "MRR = 1/N * Σ(1/rank_i)\n",
    "- N 是查询总数\n",
    "- rank_i 是第i个查询的第一个正确答案的排名\n",
    "- 如果没有正确答案，该查询的得分为0\n",
    "\n",
    "2. **举例**\n",
    "```python\n",
    "# 假设有3个查询\n",
    "# 查询1：正确答案在第1位，得分 1/1\n",
    "# 查询2：正确答案在第3位，得分 1/3\n",
    "# 查询3：正确答案在第2位，得分 1/2\n",
    "\n",
    "MRR = (1/1 + 1/3 + 1/2) / 3 = 0.611\n",
    "```\n",
    "\n",
    "这是一个计算MRR的具体实现：\n",
    "\n",
    "```python\n",
    "def calculate_mrr(self, \n",
    "                 query_embeddings: np.ndarray, \n",
    "                 corpus_embeddings: np.ndarray, \n",
    "                 relevant_docs: Dict[int, List[int]]) -> Tuple[float, List[float]]:\n",
    "    \"\"\"\n",
    "    计算MRR，并返回每个查询的具体得分\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings: 查询的嵌入向量\n",
    "        corpus_embeddings: 文档库的嵌入向量\n",
    "        relevant_docs: 字典，key是查询ID，value是相关文档ID列表\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (mrr_score, list_of_individual_scores)\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    individual_scores = []\n",
    "    \n",
    "    # 计算所有查询和文档间的相似度\n",
    "    similarities = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "    \n",
    "    for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "        # 获取相似度降序排序的文档索引\n",
    "        ranked_indices = np.argsort(similarities[query_id])[::-1]\n",
    "        \n",
    "        # 找到第一个相关文档的排名\n",
    "        rank = None\n",
    "        for pos, doc_idx in enumerate(ranked_indices, 1):\n",
    "            if doc_idx in relevant_doc_ids:\n",
    "                rank = pos\n",
    "                break\n",
    "        \n",
    "        # 计算这个查询的得分\n",
    "        if rank is not None:\n",
    "            score = 1.0 / rank\n",
    "            reciprocal_ranks.append(score)\n",
    "        else:\n",
    "            # 如果没找到相关文档，得分为0\n",
    "            score = 0.0\n",
    "            reciprocal_ranks.append(score)\n",
    "            \n",
    "        individual_scores.append({\n",
    "            'query_id': query_id,\n",
    "            'rank': rank,\n",
    "            'score': score,\n",
    "            'top_docs': ranked_indices[:5].tolist(),  # 记录前5个文档的ID\n",
    "            'top_similarities': similarities[query_id][ranked_indices[:5]].tolist()  # 记录相似度分数\n",
    "        })\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    return mrr, individual_scores\n",
    "\n",
    "# 使用示例：\n",
    "def analyze_results(self, queries, corpus, results):\n",
    "    \"\"\"\n",
    "    详细分析每个查询的表现\n",
    "    \"\"\"\n",
    "    print(\"\\nDetailed Analysis:\")\n",
    "    print(\"=================\")\n",
    "    \n",
    "    for model_key, metrics in results.items():\n",
    "        print(f\"\\n{self.model_names[model_key]}:\")\n",
    "        mrr, individual_scores = metrics['individual_scores']\n",
    "        \n",
    "        print(f\"Overall MRR: {mrr:.4f}\")\n",
    "        print(\"\\nPer-query analysis:\")\n",
    "        \n",
    "        for score_info in individual_scores:\n",
    "            query_id = score_info['query_id']\n",
    "            print(f\"\\nQuery {query_id}: {queries[query_id]}\")\n",
    "            print(f\"First relevant doc rank: {score_info['rank']}\")\n",
    "            print(f\"Reciprocal rank score: {score_info['score']:.4f}\")\n",
    "            \n",
    "            print(\"Top 5 retrieved documents:\")\n",
    "            for i, (doc_id, sim) in enumerate(zip(score_info['top_docs'], \n",
    "                                                score_info['top_similarities']), 1):\n",
    "                print(f\"{i}. [{sim:.4f}] {corpus[doc_id][:100]}...\")\n",
    "```\n",
    "\n",
    "MRR的特点：\n",
    "\n",
    "1. **优点**：\n",
    "- 关注第一个正确答案的位置\n",
    "- 计算简单，易于理解\n",
    "- 对排名靠前的结果更敏感\n",
    "\n",
    "2. **适用场景**：\n",
    "- 信息检索系统评估\n",
    "- 问答系统评估\n",
    "- 搜索引擎评估\n",
    "- 推荐系统评估\n",
    "\n",
    "3. **解释**：\n",
    "- MRR的值在0到1之间\n",
    "- 值越大表示性能越好\n",
    "- 1表示所有查询的正确答案都在第一位\n",
    "- 如果正确答案排名靠后，得分会很低\n",
    "\n",
    "4. **与其他指标的比较**：\n",
    "- Hit@K关注是否有正确答案在前K个结果中\n",
    "- MAP(Mean Average Precision)考虑所有相关文档的位置\n",
    "- MRR只关注第一个正确答案的位置\n",
    "\n",
    "这种细化的分析可以帮助我们：\n",
    "1. 了解模型在不同类型查询上的表现\n",
    "2. 发现模型的优势和不足\n",
    "3. 识别需要改进的地方\n",
    "4. 比较不同模型的具体表现差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 中的 cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设:\n",
    "- query_embeddings 的维度是 [num_queries, embedding_dim]\n",
    "- corpus_embeddings 的维度是 [num_documents, embedding_dim]\n",
    "\n",
    "其中：\n",
    "- num_queries 是查询的数量\n",
    "- num_documents 是文档的数量\n",
    "- embedding_dim 是向量维度（这个必须相同）\n",
    "\n",
    "举例：\n",
    "```python\n",
    "# 示例维度\n",
    "query_embeddings.shape  = (16, 384)    # 16个查询，每个embedding维度384\n",
    "corpus_embeddings.shape = (28, 384)    # 28个文档，每个embedding维度384\n",
    "\n",
    "# cosine_similarity 的结果维度\n",
    "similarities.shape = (16, 28)  # 每个查询对应每个文档的相似度\n",
    "```\n",
    "\n",
    "cosine_similarity 的计算过程：\n",
    "```python\n",
    "def cosine_similarity_explained(query_embeddings, corpus_embeddings):\n",
    "    \"\"\"\n",
    "    演示余弦相似度的计算过程\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings: shape [num_queries, embedding_dim]\n",
    "        corpus_embeddings: shape [num_documents, embedding_dim]\n",
    "    \n",
    "    Returns:\n",
    "        similarities: shape [num_queries, num_documents]\n",
    "    \"\"\"\n",
    "    # 1. 计算 L2 范数 (向量长度)\n",
    "    query_norms = np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "    corpus_norms = np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # 2. 归一化向量\n",
    "    query_normalized = query_embeddings / query_norms\n",
    "    corpus_normalized = corpus_embeddings / corpus_norms\n",
    "    \n",
    "    # 3. 矩阵乘法计算相似度\n",
    "    # (num_queries, embedding_dim) @ (embedding_dim, num_documents)\n",
    "    # = (num_queries, num_documents)\n",
    "    similarities = np.dot(query_normalized, corpus_normalized.T)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# 使用示例\n",
    "num_queries = 3\n",
    "num_documents = 4\n",
    "embedding_dim = 5\n",
    "\n",
    "query_embeddings = np.random.rand(num_queries, embedding_dim)\n",
    "corpus_embeddings = np.random.rand(num_documents, embedding_dim)\n",
    "\n",
    "similarities = cosine_similarity_explained(query_embeddings, corpus_embeddings)\n",
    "print(\"Query embeddings shape:\", query_embeddings.shape)\n",
    "print(\"Corpus embeddings shape:\", corpus_embeddings.shape)\n",
    "print(\"Similarities shape:\", similarities.shape)\n",
    "```\n",
    "\n",
    "关键点：\n",
    "1. embedding_dim 必须相同，因为这是同一个向量空间\n",
    "2. num_queries 和 num_documents 可以不同\n",
    "3. 最终得到的相似度矩阵维度是 [num_queries, num_documents]\n",
    "4. 每个元素 similarities[i,j] 表示第i个查询与第j个文档的余弦相似度\n",
    "\n",
    "在我们的评估代码中：\n",
    "```python\n",
    "def calculate_mrr(self, query_embeddings, corpus_embeddings, relevant_docs):\n",
    "    # similarities[i,j] 是第i个查询和第j个文档的相似度\n",
    "    similarities = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "    \n",
    "    for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "        # 对于每个查询，获取所有文档的相似度排序\n",
    "        ranked_indices = np.argsort(similarities[query_id])[::-1]\n",
    "        # similarities[query_id] 是一个长度为 num_documents 的向量\n",
    "```\n",
    "\n",
    "这也解释了为什么 similarities[query_id] 可以得到特定查询与所有文档的相似度向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10553585, 0.1955418 , 0.85905077, 0.50188372, 0.96730807],\n",
       "       [0.61035641, 0.22100457, 0.46646891, 0.58499486, 0.94678857],\n",
       "       [0.60733863, 0.30711586, 0.93369464, 0.47716306, 0.04734569]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5731121 , 0.02161613, 0.80159162, 0.8401583 , 0.80807207],\n",
       "       [0.51913204, 0.0789993 , 0.85645666, 0.44378902, 0.92486281],\n",
       "       [0.39858005, 0.98111986, 0.8457453 , 0.46485596, 0.8464015 ],\n",
       "       [0.70138594, 0.77555282, 0.01866932, 0.52679028, 0.65922755]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5731121 , 0.51913204, 0.39858005, 0.70138594],\n",
       "       [0.02161613, 0.0789993 , 0.98111986, 0.77555282],\n",
       "       [0.80159162, 0.85645666, 0.8457453 , 0.01866932],\n",
       "       [0.8401583 , 0.44378902, 0.46485596, 0.52679028],\n",
       "       [0.80807207, 0.92486281, 0.8464015 , 0.65922755]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91206929, 0.95319192, 0.86055598, 0.60565383],\n",
       "       [0.94899571, 0.9500114 , 0.84549014, 0.8366131 ],\n",
       "       [0.8076444 , 0.77653305, 0.7661669 , 0.57354788]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91206929, 0.95319192, 0.86055598, 0.60565383])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120692908380097"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "==================\n",
      "\n",
      "Point:\n",
      "Processing time: 0.06 seconds\n",
      "MRR: 0.9583\n",
      "Hit@1: 0.9375\n",
      "Hit@3: 1.0000\n",
      "Hit@5: 1.0000\n",
      "Hit@10: 1.0000\n",
      "\n",
      "Cona:\n",
      "Processing time: 0.08 seconds\n",
      "MRR: 0.9688\n",
      "Hit@1: 0.9375\n",
      "Hit@3: 1.0000\n",
      "Hit@5: 1.0000\n",
      "Hit@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self):\n",
    "        self.model1 = SentenceTransformer('/root/app/models/point_large_embedding_zh')\n",
    "        self.model2 = SentenceTransformer('/root/app/models/Conan-embedding-v1')\n",
    "        \n",
    "        self.model_names = {\n",
    "            'model1': 'Point',\n",
    "            'model2': 'Cona'\n",
    "        }\n",
    "\n",
    "    def compute_embeddings(self, texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "        return model.encode(texts)\n",
    "\n",
    "    def calculate_mrr(self, \n",
    "                     query_embeddings: np.ndarray, \n",
    "                     corpus_embeddings: np.ndarray, \n",
    "                     relevant_docs: Dict[int, List[int]]) -> float:\n",
    "        \"\"\"\n",
    "        计算 Mean Reciprocal Rank\n",
    "        relevant_docs: 字典，key是查询ID，value是相关文档ID列表\n",
    "        \"\"\"\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        # 计算余弦相似度\n",
    "        similarities = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "        \n",
    "        for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "            # 获取相似度排序的索引\n",
    "            ranked_indices = np.argsort(similarities[query_id])[::-1]\n",
    "            \n",
    "            # 找到第一个相关文档的位置\n",
    "            for rank, doc_idx in enumerate(ranked_indices, 1):\n",
    "                if doc_idx in relevant_doc_ids:\n",
    "                    reciprocal_ranks.append(1.0 / rank)\n",
    "                    break\n",
    "                    \n",
    "        return np.mean(reciprocal_ranks)\n",
    "\n",
    "    def calculate_hit_rate(self, \n",
    "                          query_embeddings: np.ndarray, \n",
    "                          corpus_embeddings: np.ndarray, \n",
    "                          relevant_docs: Dict[int, List[int]], \n",
    "                          k: int) -> float:\n",
    "        \"\"\"\n",
    "        计算 Hit@k\n",
    "        \"\"\"\n",
    "        hits = 0\n",
    "        total_queries = len(relevant_docs)\n",
    "        \n",
    "        similarities = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "        \n",
    "        for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "            # 获取前k个最相似文档的索引\n",
    "            top_k_indices = np.argsort(similarities[query_id])[::-1][:k]\n",
    "            \n",
    "            # 检查是否有相关文档在前k个结果中\n",
    "            if any(idx in relevant_doc_ids for idx in top_k_indices):\n",
    "                hits += 1\n",
    "                \n",
    "        return hits / total_queries\n",
    "\n",
    "    def evaluate_models(self, \n",
    "                       queries: List[str], \n",
    "                       corpus: List[str], \n",
    "                       relevant_docs: Dict[int, List[int]], \n",
    "                       k_values: List[int]) -> Dict:\n",
    "        results = {\n",
    "            'model1': defaultdict(dict),\n",
    "            'model2': defaultdict(dict)\n",
    "        }\n",
    "        \n",
    "        for model_key, model in [('model1', self.model1), ('model2', self.model2)]:\n",
    "            # 计算嵌入向量\n",
    "            start_time = time.time()\n",
    "            query_embeddings = self.compute_embeddings(queries, model)\n",
    "            corpus_embeddings = self.compute_embeddings(corpus, model)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # 计算 MRR\n",
    "            mrr = self.calculate_mrr(query_embeddings, corpus_embeddings, relevant_docs)\n",
    "            \n",
    "            # 计算不同k值的Hit@k\n",
    "            hit_rates = {\n",
    "                k: self.calculate_hit_rate(query_embeddings, corpus_embeddings, \n",
    "                                         relevant_docs, k) \n",
    "                for k in k_values\n",
    "            }\n",
    "            \n",
    "            results[model_key] = {\n",
    "                'processing_time': processing_time,\n",
    "                'mrr': mrr,\n",
    "                'hit_rates': hit_rates\n",
    "            }\n",
    "            \n",
    "        return results\n",
    "\n",
    "def get_test_data():\n",
    "    # 测试数据集包含以下几个挑战类别：\n",
    "    # 1. 跨语言相似性\n",
    "    # 2. 同义词和近义词\n",
    "    # 3. 抽象概念\n",
    "    # 4. 专业领域知识\n",
    "    # 5. 长文本与短文本\n",
    "    # 6. 隐含语义\n",
    "    \n",
    "    queries = [\n",
    "        # 跨语言查询\n",
    "        \"What is machine learning\",\n",
    "        \"如何实现数据可视化\",\n",
    "        \"deep learning applications\",\n",
    "        \"自然语言处理的发展趋势\",\n",
    "        \n",
    "        # 专业领域查询\n",
    "        \"BERT模型的原理\",\n",
    "        \"Transformer architecture explained\",\n",
    "        \"GPU vs TPU performance comparison\",\n",
    "        \"分布式训练策略\",\n",
    "        \n",
    "        # 抽象概念查询\n",
    "        \"人工智能的伦理问题\",\n",
    "        \"The future of autonomous systems\",\n",
    "        \"数据隐私保护方法\",\n",
    "        \"Sustainable AI development\",\n",
    "        \n",
    "        # 具体技术查询\n",
    "        \"PyTorch实现多GPU训练\",\n",
    "        \"Kubernetes deployment best practices\",\n",
    "        \"优化神经网络训练速度\",\n",
    "        \"Implementing attention mechanism\"\n",
    "    ]\n",
    "    \n",
    "    corpus = [\n",
    "        # 机器学习基础解释\n",
    "        \"Machine learning is a subset of artificial intelligence that focuses on data and algorithms\",\n",
    "        \"机器学习是人工智能的一个子集，主要关注数据和算法\",\n",
    "        \"深度学习是机器学习的一个分支，使用多层神经网络进行学习\",\n",
    "        \"Deep learning is a branch of machine learning using multi-layer neural networks\",\n",
    "        \n",
    "        # 数据可视化相关\n",
    "        \"Data visualization techniques include charts, graphs, and interactive dashboards\",\n",
    "        \"数据可视化技术包括图表、图形和交互式仪表板\",\n",
    "        \"使用Python的Matplotlib和Seaborn库进行数据可视化\",\n",
    "        \"Advanced data visualization can be achieved using D3.js and WebGL\",\n",
    "        \n",
    "        # BERT和Transformer相关\n",
    "        \"BERT uses bidirectional transformer architecture to understand context\",\n",
    "        \"BERT模型通过双向Transformer架构来理解上下文语义\",\n",
    "        \"Transformer architecture relies heavily on self-attention mechanisms\",\n",
    "        \"Transformer架构主要依赖于自注意力机制\",\n",
    "        \n",
    "        # 硬件和性能优化\n",
    "        \"GPUs excel at parallel processing while TPUs are optimized for tensor operations\",\n",
    "        \"GPU适合并行处理，而TPU针对张量运算进行了优化\",\n",
    "        \"分布式训练可以显著提高大规模模型的训练效率\",\n",
    "        \"Distributed training can significantly improve the efficiency of large-scale models\",\n",
    "        \n",
    "        # AI伦理和未来发展\n",
    "        \"AI ethics concerns include bias, privacy, and accountability\",\n",
    "        \"人工智能伦理问题包括偏见、隐私和责任归属\",\n",
    "        \"Autonomous systems must balance efficiency with safety and reliability\",\n",
    "        \"自动驾驶系统需要在效率和安全性之间取得平衡\",\n",
    "        \n",
    "        # 技术实现细节\n",
    "        \"PyTorch provides DataParallel and DistributedDataParallel for multi-GPU training\",\n",
    "        \"PyTorch提供了DataParallel和DistributedDataParallel用于多GPU训练\",\n",
    "        \"Kubernetes可以有效管理和扩展机器学习工作负载\",\n",
    "        \"Kubernetes can effectively manage and scale machine learning workloads\",\n",
    "        \n",
    "        # 神经网络优化\n",
    "        \"Neural network optimization techniques include gradient clipping and batch normalization\",\n",
    "        \"神经网络优化技术包括梯度裁剪和批量归一化\",\n",
    "        \"注意力机制通过权重计算来关注重要特征\",\n",
    "        \"Attention mechanisms focus on important features through weight calculations\"\n",
    "    ]\n",
    "    \n",
    "    # 相关文档映射：查询ID -> 相关文档ID列表\n",
    "    relevant_docs = {\n",
    "        0: [0, 1, 2, 3],           # What is machine learning\n",
    "        1: [4, 5, 6, 7],           # 数据可视化\n",
    "        2: [2, 3],                 # deep learning applications\n",
    "        3: [2, 3, 10, 11],         # 自然语言处理发展趋势\n",
    "        4: [8, 9, 10, 11],         # BERT模型原理\n",
    "        5: [10, 11],               # Transformer architecture\n",
    "        6: [12, 13],               # GPU vs TPU\n",
    "        7: [14, 15],               # 分布式训练\n",
    "        8: [16, 17],               # AI伦理\n",
    "        9: [18, 19],               # autonomous systems\n",
    "        10: [16, 17],              # 数据隐私\n",
    "        11: [16, 17, 18, 19],      # Sustainable AI\n",
    "        12: [20, 21],              # PyTorch多GPU\n",
    "        13: [22, 23],              # Kubernetes\n",
    "        14: [24, 25],              # 优化神经网络\n",
    "        15: [26, 27]               # attention mechanism\n",
    "    }\n",
    "    \n",
    "    return queries, corpus, relevant_docs\n",
    "\n",
    "def main():\n",
    "    queries, corpus, relevant_docs = get_test_data()\n",
    "    k_values = [1, 3, 5, 10]\n",
    "    \n",
    "    evaluator = EmbeddingEvaluator()\n",
    "    results = evaluator.evaluate_models(queries, corpus, relevant_docs, k_values)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"==================\")\n",
    "    \n",
    "    for model_key, metrics in results.items():\n",
    "        print(f\"\\n{evaluator.model_names[model_key]}:\")\n",
    "        print(f\"Processing time: {metrics['processing_time']:.2f} seconds\")\n",
    "        print(f\"MRR: {metrics['mrr']:.4f}\")\n",
    "        for k, hit_rate in metrics['hit_rates'].items():\n",
    "            print(f\"Hit@{k}: {hit_rate:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取 JSON 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '6'\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self):\n",
    "        self.model1 = SentenceTransformer('/root/app/models/Conan-embedding-v1')\n",
    "        self.model2 = SentenceTransformer('/root/app/models/bge-m3')\n",
    "        \n",
    "        self.model_names = {\n",
    "            'model1': 'Conan',\n",
    "            'model2': 'BGE-M3'\n",
    "        }\n",
    "\n",
    "    def compute_embeddings(self, texts: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "        return model.encode(texts)\n",
    "\n",
    "    def calculate_mrr(self, \n",
    "                     query_embeddings: np.ndarray, \n",
    "                     corpus_embeddings: np.ndarray, \n",
    "                     relevant_docs: Dict[int, List[int]]) -> float:\n",
    "        \"\"\"\n",
    "        计算 Mean Reciprocal Rank\n",
    "        relevant_docs: 字典，key是查询ID，value是相关文档ID列表\n",
    "        \"\"\"\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        # 计算余弦相似度\n",
    "        similarities = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "        \n",
    "        for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "            # 获取相似度排序的索引\n",
    "            ranked_indices = np.argsort(similarities[query_id])[::-1]\n",
    "            \n",
    "            # 找到第一个相关文档的位置\n",
    "            for rank, doc_idx in enumerate(ranked_indices, 1):\n",
    "                if doc_idx in relevant_doc_ids:\n",
    "                    reciprocal_ranks.append(1.0 / rank)\n",
    "                    break\n",
    "                    \n",
    "        return np.mean(reciprocal_ranks)\n",
    "\n",
    "    def calculate_hit_rate(self, \n",
    "                          query_embeddings: np.ndarray, \n",
    "                          corpus_embeddings: np.ndarray, \n",
    "                          relevant_docs: Dict[int, List[int]], \n",
    "                          k: int) -> float:\n",
    "        \"\"\"\n",
    "        计算 Hit@k\n",
    "        \"\"\"\n",
    "        hits = 0\n",
    "        total_queries = len(relevant_docs)\n",
    "        \n",
    "        similarities = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "        \n",
    "        for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "            # 获取前k个最相似文档的索引\n",
    "            top_k_indices = np.argsort(similarities[query_id])[::-1][:k]\n",
    "            \n",
    "            # 检查是否有相关文档在前k个结果中\n",
    "            if any(idx in relevant_doc_ids for idx in top_k_indices):\n",
    "                hits += 1\n",
    "                \n",
    "        return hits / total_queries\n",
    "\n",
    "    def evaluate_models(self, \n",
    "                       queries: List[str], \n",
    "                       corpus: List[str], \n",
    "                       relevant_docs: Dict[int, List[int]], \n",
    "                       k_values: List[int]) -> Dict:\n",
    "        results = {\n",
    "            'model1': defaultdict(dict),\n",
    "            'model2': defaultdict(dict)\n",
    "        }\n",
    "        \n",
    "        for model_key, model in tqdm([('model1', self.model1), ('model2', self.model2)], \n",
    "                                desc=\"评估模型\", position=0):\n",
    "            # 计算嵌入向量\n",
    "            start_time = time.time()\n",
    "            query_embeddings = self.compute_embeddings(queries, model)\n",
    "            corpus_embeddings = self.compute_embeddings(corpus, model)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # 计算 MRR\n",
    "            mrr = self.calculate_mrr(query_embeddings, corpus_embeddings, relevant_docs)\n",
    "            \n",
    "            # 计算不同k值的Hit@k\n",
    "            hit_rates = {\n",
    "                k: self.calculate_hit_rate(query_embeddings, corpus_embeddings, \n",
    "                                         relevant_docs, k) \n",
    "                for k in k_values\n",
    "            }\n",
    "            \n",
    "            results[model_key] = {\n",
    "                'processing_time': processing_time,\n",
    "                'mrr': mrr,\n",
    "                'hit_rates': hit_rates\n",
    "            }\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    # 测试数据集包含以下几个挑战类别：\n",
    "    # 1. 跨语言相似性\n",
    "    # 2. 同义词和近义词\n",
    "    # 3. 抽象概念\n",
    "    # 4. 专业领域知识\n",
    "    # 5. 长文本与短文本\n",
    "    # 6. 隐含语义\n",
    "    \n",
    "    queries = [\n",
    "        # 跨语言查询\n",
    "        \"What is machine learning\",\n",
    "        \"如何实现数据可视化\",\n",
    "        \"deep learning applications\",\n",
    "        \"自然语言处理的发展趋势\",\n",
    "        \n",
    "        # 专业领域查询\n",
    "        \"BERT模型的原理\",\n",
    "        \"Transformer architecture explained\",\n",
    "        \"GPU vs TPU performance comparison\",\n",
    "        \"分布式训练策略\",\n",
    "        \n",
    "        # 抽象概念查询\n",
    "        \"人工智能的伦理问题\",\n",
    "        \"The future of autonomous systems\",\n",
    "        \"数据隐私保护方法\",\n",
    "        \"Sustainable AI development\",\n",
    "        \n",
    "        # 具体技术查询\n",
    "        \"PyTorch实现多GPU训练\",\n",
    "        \"Kubernetes deployment best practices\",\n",
    "        \"优化神经网络训练速度\",\n",
    "        \"Implementing attention mechanism\"\n",
    "    ]\n",
    "    \n",
    "    corpus = [\n",
    "        # 机器学习基础解释\n",
    "        \"Machine learning is a subset of artificial intelligence that focuses on data and algorithms\",\n",
    "        \"机器学习是人工智能的一个子集，主要关注数据和算法\",\n",
    "        \"深度学习是机器学习的一个分支，使用多层神经网络进行学习\",\n",
    "        \"Deep learning is a branch of machine learning using multi-layer neural networks\",\n",
    "        \n",
    "        # 数据可视化相关\n",
    "        \"Data visualization techniques include charts, graphs, and interactive dashboards\",\n",
    "        \"数据可视化技术包括图表、图形和交互式仪表板\",\n",
    "        \"使用Python的Matplotlib和Seaborn库进行数据可视化\",\n",
    "        \"Advanced data visualization can be achieved using D3.js and WebGL\",\n",
    "        \n",
    "        # BERT和Transformer相关\n",
    "        \"BERT uses bidirectional transformer architecture to understand context\",\n",
    "        \"BERT模型通过双向Transformer架构来理解上下文语义\",\n",
    "        \"Transformer architecture relies heavily on self-attention mechanisms\",\n",
    "        \"Transformer架构主要依赖于自注意力机制\",\n",
    "        \n",
    "        # 硬件和性能优化\n",
    "        \"GPUs excel at parallel processing while TPUs are optimized for tensor operations\",\n",
    "        \"GPU适合并行处理，而TPU针对张量运算进行了优化\",\n",
    "        \"分布式训练可以显著提高大规模模型的训练效率\",\n",
    "        \"Distributed training can significantly improve the efficiency of large-scale models\",\n",
    "        \n",
    "        # AI伦理和未来发展\n",
    "        \"AI ethics concerns include bias, privacy, and accountability\",\n",
    "        \"人工智能伦理问题包括偏见、隐私和责任归属\",\n",
    "        \"Autonomous systems must balance efficiency with safety and reliability\",\n",
    "        \"自动驾驶系统需要在效率和安全性之间取得平衡\",\n",
    "        \n",
    "        # 技术实现细节\n",
    "        \"PyTorch provides DataParallel and DistributedDataParallel for multi-GPU training\",\n",
    "        \"PyTorch提供了DataParallel和DistributedDataParallel用于多GPU训练\",\n",
    "        \"Kubernetes可以有效管理和扩展机器学习工作负载\",\n",
    "        \"Kubernetes can effectively manage and scale machine learning workloads\",\n",
    "        \n",
    "        # 神经网络优化\n",
    "        \"Neural network optimization techniques include gradient clipping and batch normalization\",\n",
    "        \"神经网络优化技术包括梯度裁剪和批量归一化\",\n",
    "        \"注意力机制通过权重计算来关注重要特征\",\n",
    "        \"Attention mechanisms focus on important features through weight calculations\"\n",
    "    ]\n",
    "    \n",
    "    # 相关文档映射：查询ID -> 相关文档ID列表\n",
    "    relevant_docs = {\n",
    "        0: [0, 1, 2, 3],           # What is machine learning\n",
    "        1: [4, 5, 6, 7],           # 数据可视化\n",
    "        2: [2, 3],                 # deep learning applications\n",
    "        3: [2, 3, 10, 11],         # 自然语言处理发展趋势\n",
    "        4: [8, 9, 10, 11],         # BERT模型原理\n",
    "        5: [10, 11],               # Transformer architecture\n",
    "        6: [12, 13],               # GPU vs TPU\n",
    "        7: [14, 15],               # 分布式训练\n",
    "        8: [16, 17],               # AI伦理\n",
    "        9: [18, 19],               # autonomous systems\n",
    "        10: [16, 17],              # 数据隐私\n",
    "        11: [16, 17, 18, 19],      # Sustainable AI\n",
    "        12: [20, 21],              # PyTorch多GPU\n",
    "        13: [22, 23],              # Kubernetes\n",
    "        14: [24, 25],              # 优化神经网络\n",
    "        15: [26, 27]               # attention mechanism\n",
    "    }\n",
    "    \n",
    "    return queries, corpus, relevant_docs\n",
    "\n",
    "def main():\n",
    "    import json\n",
    "    json_path = \"/root/app/en-unshuffle_combined_data.json\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    queries = [item['query'] for item in data]\n",
    "    corpus = [item['chunk'] for item in data]\n",
    "    relevant_docs = {i: [j for j in range(len(data)) if i == j] for i in range(len(data))}\n",
    "\n",
    "    # queries, corpus, relevant_docs = get_test_data()\n",
    "    k_values = [1, 3, 5, 10]\n",
    "    \n",
    "    evaluator = EmbeddingEvaluator()\n",
    "    results = evaluator.evaluate_models(queries, corpus, relevant_docs, k_values)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"==================\")\n",
    "    \n",
    "    for model_key, metrics in results.items():\n",
    "        print(f\"\\n{evaluator.model_names[model_key]}:\")\n",
    "        print(f\"Processing time: {metrics['processing_time']:.2f} seconds\")\n",
    "        print(f\"MRR: {metrics['mrr']:.4f}\")\n",
    "        for k, hit_rate in metrics['hit_rates'].items():\n",
    "            print(f\"Hit@{k}: {hit_rate:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
